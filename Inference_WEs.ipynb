{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pedrotajia/ENTER/envs/AISC_B_project/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/Users/pedrotajia/ENTER/envs/AISC_B_project/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.2' currently installed).\n",
      "  from pandas.core import (\n",
      "/Users/pedrotajia/ENTER/envs/AISC_B_project/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import lightgbm\n",
    "import numpy as np\n",
    "from ollama import chat\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SentenceTransformer instance with the stella_en_1.5B_v5 model\n",
    "word_embedding = SentenceTransformer(\n",
    "    \"dunzhang/stella_en_1.5B_v5\",     # The name/identifier of the Hugging Face model \n",
    "    device=\"mps\",                     # Use Apple's Metal Performance Shaders (MPS) for GPU acceleration on Apple Silicon\n",
    "    config_kwargs={\"use_memory_efficient_attention\": False},  # Disable memory-efficient attention to ensure compatibility\n",
    "    trust_remote_code=True            # Allow execution of custom code from the model's repository\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text2Embedding():\n",
    "    def __init__(self, model):\n",
    "        \"\"\"\n",
    "        The constructor takes in a text-embedding model object (e.g., \n",
    "        a SentenceTransformer instance or any other encoding model).\n",
    "        \n",
    "        :param model: A text embedding model with a .encode() method.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "\n",
    "    def sentence2vector(self, sentences, name='product_description'):\n",
    "        \"\"\"\n",
    "        Encodes a given text (or list of texts) into an embedding.\n",
    "        \n",
    "        :param sentences: The text input(s) to encode.\n",
    "        :param name: Optional identifier for debugging/logging purposes.\n",
    "        :return: The raw embedding vector (or a list of vectors).\n",
    "        \"\"\"\n",
    "        print(f\"Encoding sentences for feature '{name}'...\")\n",
    "        vector = self.model.encode(sentences)\n",
    "        return vector\n",
    "\n",
    "    def transform(self, input):\n",
    "        \"\"\"\n",
    "        A convenience method that calls sentence2vector() and reshapes \n",
    "        the resulting embedding(s) to a fixed shape of (1, 1024). \n",
    "        This implies the model outputs a 1024-dimensional embedding.\n",
    "        \n",
    "        :param input: The text input to encode.\n",
    "        :return: A (1, 1024) NumPy array representing the single text embedding.\n",
    "        \"\"\"\n",
    "        embedding = self.sentence2vector(input)\n",
    "        return embedding.reshape(1, 1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from 'model_gbm.txt' file\n",
    "\n",
    "gbm = lightgbm.Booster(model_file='model_gbm.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding sentences for feature 'product_description'...\n",
      "Encoding sentences for feature 'product_description'...\n",
      "Iteration 1: Old Score=0.4462, New Score=-0.0014\n",
      "Encoding sentences for feature 'product_description'...\n",
      "Iteration 2: Old Score=0.4462, New Score=0.2919\n",
      "Encoding sentences for feature 'product_description'...\n",
      "Iteration 3: Old Score=0.4462, New Score=0.2602\n",
      "Encoding sentences for feature 'product_description'...\n",
      "Iteration 4: Old Score=0.4462, New Score=0.2523\n",
      "Encoding sentences for feature 'product_description'...\n",
      "Iteration 5: Old Score=0.4462, New Score=-0.0596\n",
      "Encoding sentences for feature 'product_description'...\n",
      "Iteration 6: Old Score=0.4462, New Score=-0.0444\n",
      "\n",
      "Old description: ezrxghgvhg good not yttcrtet\n",
      "\n",
      "Best description: \n",
      "This exquisite handbag boasts a majestic shoulder height of 94cm, perfectly proportioned for daily use. Its timeless design has been meticulously restored to perfection, featuring a rich, dark brown leather exterior that showcases a striking contrast with premium silver accents. The interior lining remains largely intact, maintaining its classic allure. A delicate satin ribbon adds a touch of understated elegance to the zipper, complementing the overall sophisticated and refined aesthetic. Measuring 20cm in length and 13cm in width, this stylish accessory exudes refinement and poise, making it an ideal choice for those seeking a perfect blend of functionality and luxury, perfect for formal or everyday occasions alike.\n",
      "\n",
      "Best score: 0.4462181540028869\n",
      "\n",
      "Percent of improvment: 0.000%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the initial description text\n",
    "example = \"Authentic vintage Chanel made out of luxurious black lambskin. Featuring gold CC closure. Size W: 25cm H: 17cm Size D: 2cm. Shoulder height: 94cm. Inside lining has been fully replaced. Comes with ribbon.\"\n",
    "# Transform the text into an embedding \n",
    "transform_embedding = Text2Embedding(word_embedding)\n",
    "example_embedding = transform_embedding.transform(example)\n",
    "\n",
    "# Define a threshold for the score and the maximum number of iterations\n",
    "score_threshold = 0.9\n",
    "max_iterations = 6\n",
    "\n",
    "# Use a GBM model to predict the \"score\" for the current description\n",
    "best_score = gbm.predict(example_embedding)\n",
    "best_score = float(best_score[0])\n",
    "\n",
    "# Keep track of the first (original) description and its score\n",
    "first_example = example\n",
    "first_score = best_score\n",
    "\n",
    "# Start iterating until we meet our threshold or reach the max_iterations\n",
    "i = 1\n",
    "while best_score < score_threshold and i <= max_iterations:\n",
    "    # Prepare the prompt as messages for the chat model\n",
    "    #  - The system message instructs the model to improve the description\n",
    "    #  - The user message includes the current score and the text to improve\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You task is to improve the description based on a 'score', \"\n",
    "                \"bigger 'score' is better, also keep all the essential information \"\n",
    "                \"(sizes, colors, etc) anything especific. \"\n",
    "                \"Note just return the 'Description'\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Score: {best_score:.2f} | Description: {example} \"\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # Call the chat model to get a new/improved description\n",
    "    response = chat(model=\"llama3.2\", messages=messages)\n",
    "    \n",
    "    # Extract the new description from the response\n",
    "    # Assumes the response content is in the format: \"Description: ...\"\n",
    "    example = response['message']['content'].split(':')[-1]\n",
    "\n",
    "    # Transform the new description to get its embedding\n",
    "    example_embedding = transform_embedding.transform(example)\n",
    "    \n",
    "    # Predict the score of this new description\n",
    "    new_score = gbm.predict(example_embedding)\n",
    "    new_score = float(new_score[0])\n",
    "\n",
    "    # Print debug information: how the score changed from old to new\n",
    "    print(f\"Iteration {i}: Old Score={best_score:.4f}, New Score={new_score:.4f}\")\n",
    "\n",
    "    # If the new score is better, update our best_score and best_text\n",
    "    if new_score > best_score:\n",
    "        best_score = new_score\n",
    "        best_text = example\n",
    "\n",
    "    i += 1  # Move to the next iteration\n",
    "\n",
    "# Once we exit the loop, track the final score\n",
    "last_score = best_score\n",
    "\n",
    "# Calculate the percentage improvement over the first/original score\n",
    "percent_of_change = (((last_score - first_score) / first_score) * 100)\n",
    "\n",
    "# Print out the old and best descriptions with their corresponding scores\n",
    "print(f'\\nOld description: {first_example}')\n",
    "print(f\"\\nBest description: {best_text}\")\n",
    "print(f\"\\nBest score: {best_score}\")\n",
    "print(f\"\\nPercent of improvment: {percent_of_change:.3f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AISC_C_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
